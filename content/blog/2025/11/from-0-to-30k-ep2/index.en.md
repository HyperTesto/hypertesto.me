---
title: "From 0 to 30K - Numbers"
date: 2025-11-06T17:51:32+01:00
description: "A theoretical journey of distributed systems through the lens of Star Citizen"
draft: false
categories:
- gaming
tags:
- star citizen
- networks
- simulations
series:
- From 0 to 30K
authors: "hypertesto"
---
> Numbers are like stars: infinite in theory, but you only see a handful at a time

Welcome to Episodeâ€¯2 of the â€œComputerâ€‘Scienceâ€‘throughâ€‘Starâ€¯Citizenâ€ series. After dissecting latency, jitter, and bandwidth in the first post,
we now turn to the **foundation of every computation** â€“ how a computer actually stores numbers. Whether youâ€™re tracking a shipâ€™s velocity,
calculating shield regeneration, or tallying inâ€‘game credits, the underlying numeric representation dictates whatâ€™s possible, what can go wrong,
and how much bandwidth youâ€™ll burn. Mastering these basics will give you the mental toolkit for every deeper dive weâ€™ll take later (procedural planets,
AI pathâ€‘finding, network sync, you name it).

## Integer Basics

Every datum in a computer is a string of **bits** (`0` or `1`). With *n* bits you can encode `2â¿` distinct values.

If you treat all *n* bits as **unsigned**, the largest value you can represent is:

| Bits | Max unsigned value |
|------|-------------------|
| 8    | 255               |
| 16   | 65â€¯535            |
| 32   | 4â€¯294â€¯967â€¯295     |
| 64   | 18â€¯446â€¯744â€¯073â€¯709â€¯551â€¯615 |

In Starâ€¯Citizen terms, an unsigned 32â€‘bit counter could theoretically track **over four billion** cargo units â€“ far more than any ship could ever carry.

Most gameplay data needs to represent both positive and negative quantities (damage, velocity, score deltas, etc.).
To do that we reserve **one bit for the sign** and use **twoâ€™sâ€‘complement** for the negative range. The resulting range becomes:

| Bits | Signed range (min â€¦ max) |
|------|--------------------------|
| 8    | â€“128 â€¦ 127               |
| 16   | â€“32â€¯768 â€¦ 32â€¯767         |
| 32   | â€“2â€¯147â€¯483â€¯648 â€¦ 2â€¯147â€¯483â€¯647 |
| 64   | â€“9â€¯223â€¯372â€¯036â€¯854â€¯775â€¯808 â€¦ 9â€¯223â€¯372â€¯036â€¯854â€¯775â€¯807 |

So a **signed 32â€‘bit integer**â€”the type most of the gameâ€™s counters useâ€”can hold roughly **Â±2â€¯billion**.
Thatâ€™s plenty for most inâ€‘game values, but it *is* a hard ceiling; once you exceed it you get the classic overflow behaviour (the value wraps around to the opposite extreme).

Adding `1` to the largest positive signed value flips it to the most negative one[^0]:

```127 (0b01111111) + 1 â†’ -128 (0b10000000) â† 8â€‘bit signed overflow```

Thatâ€™s why a badlyâ€‘coded cargo counter can suddenly display â€œâ€‘1â€¯tonâ€ and make you wonder if the game has entered a black hole.

{{< figure src="int_ranges.png" alt="Integer ranges" caption="Maximum signed/unsigned integer values for common bitâ€‘widths." >}}

Note that the y axis in in **logarithmic scale**[^log] or the bars representing 8, 16 and 32 bits would be insignificant.
This emphasizes the exponential growth of the maximum representable value as the bitâ€‘width increases.

### A quick byte primer
In few places the article will mentions â€œbytesâ€, they are not some kind of wizardry spell, but a name to call "a group of 8 bits".

A byte is the smallest addressable unit of memory in virtually every modern computer.
When we say â€œa 32â€‘bit float occupies 4â€¯bytes,â€ we mean the value is stored in a contiguous block of four bytes in RAM or in a network packet.
Reducing the number of bytes per field (e.g., using a 16â€‘bit fixedâ€‘point integer instead of a 32â€‘bit float) cuts memory usage and network traffic roughly in half.

### More bits, more problems?

Itâ€™s tempting to say, â€œJust switch from a 32â€‘bit `int` to a 64â€‘bit `long` and weâ€™ll never run out of cargo slots again.â€ Technically that **does** raise the theoretical limit from ~2â€¯billion to ~9â€¯quintillion, which is comfortably larger than any conceivable inâ€‘game inventory.

But the cure isnâ€™t free:

- **Memory footprint** â€“ each extra 32â€¯bits per field means every entity that carries that field consumes four additional bytes. Multiply that by the tens of thousands of objects the server tracks (ships, missiles, debris, UI elements) and you quickly
add **hundreds of megabytes** to the baseline memory usage.
- **Network bandwidth** â€“ the server periodically streams position, health, and status packets to every client.
Doubling the size of those numeric payloads roughly **doubles the outbound traffic** for those fields, which can push you past the comfortable margin on slower connections and increase packet loss.
- **Cache pressure** â€“ modern CPUs rely on L1/L2 caches that hold only a few megabytes.
Larger structures mean fewer objects fit in cache, leading to more cache misses and a measurable hit to frameâ€‘rate, especially during dense combat scenes.

So while a `long` solves the *finiteâ€‘range* headache, it **creates a new set of performance and scalability challenges** that the engine must mitigate (e.g., by compressing packets, using deltaâ€‘encoding, or falling back to relative offsets).
Itâ€™s a classic engineering tradeâ€‘off: *more precision â†’ more resources*.

## Floating-point numbers

Most scientific values â€“ ship velocity, fuel consumption, laser damage â€“ live in the realm of **real numbers**. Storing them exactly would require infinite bits,
so we settle for the **IEEEâ€‘754 floatingâ€‘point**[^1] formatwhich splits the bits into three logical parts:

| Part          | What it stores                                                            | How it contributes to the value                                                                                       |
|---------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| **Sign bit**  | `0`â€¯=â€¯positive, `1`â€¯=â€¯negative                                          | Determines the overall sign of the number (`(-1)^sign`).                                                              |
| **Exponent**  | Unsigned integer with a *bias* (e.g., 127 for 32â€‘bit, 1023 for 64â€‘bit)   | Sets the scale: `2^(exponent âˆ’ bias)`. Controls the magnitude (large vs. tiny numbers).                             |
| **Mantissa**  | Binary fraction (23 bits for singleâ€‘precision, 52 bits for doubleâ€‘precision) | Supplies the precision: the value is `1.mantissa` in binary. More mantissa bits â†’ higher precision.                  |

Putting it together, the numeric value is:

> {{< math >}}\text{number} = (-1)^{\text{sign}} \times 2^{(\text{exponent} - \text{bias})} \times (1.\text{mantissa}){{< /math >}}


### Intuitive picture

- **Exponent â†’ â€œhow far left/right the decimal point moves.â€**
  Think of the exponent as deciding whether youâ€™re dealing with a tiny microâ€‘meter (2â»Â²â°) or a galaxyâ€‘scale distance (2Â³â°). Changing the exponent byâ€¯1 doubles or halves the magnitude.

- **Mantissa â†’ â€œthe digits you keep after the point.â€**
  The mantissa supplies the fineâ€‘grained detail. With 23 mantissa bits (single precision) you get about 7 decimal digits of precision; with 52 bits (double) you get about 15 decimal digits.

Because the exponent determines the step size, the distance between two adjacent representable numbers grows as the exponent grows. Near zero the steps are tiny (high precision);
far from zero the steps become large, which is why a double can represent enormous values but loses granularity for very large numbers.
This is exactly the behavior illustrated here.

{{< figure src="float_spacing.png" alt="Float spacing" caption="How the gap between consecutive IEEEâ€‘754 doubles widens with magnitude (logâ€‘log plot)." >}}

### Quick sanity check (Python)

```python
import struct

def float_bits(f):
    # pack a 32â€‘bit float, then unpack as unsigned int to see the bits
    return f'{struct.unpack(">I", struct.pack(">f", f))[0]:032b}'

print(float_bits(1.5))   # 00111111110000000000000000000000
# sign=0, exponent=01111111 (bias 127 â†’ actual exponent 0), mantissa=100â€¦
```

Reading the bit pattern confirms the three components: `signâ€¯=â€¯0` (positive), `exponentâ€¯=â€¯127â€¯â†’â€¯actual exponentâ€¯0`, `mantissaâ€¯=â€¯0.5` (the 100â€¦ part), giving (+1)â€¯Ã—â€¯2â°â€¯Ã—â€¯(1â€¯+â€¯0.5) = 1.5.

Another classic demonstration: `0.1 + 0.2` does **not** equal `0.3` in binary floatingâ€‘point:

```python
>>> 0.1 + 0.2
0.30000000000000004
```

In a dogfight that extra `4â€¯Ã—â€¯10â»Â¹â·` seconds can be the difference between a clean hit and a spectacular miss,
especially when the error accumulates over thousands of physics updates.

{{< figure src="float_error_accum.png" alt="Floatingâ€‘point error accumulation" caption="Even after a thousand additions of 0.1, the accumulated error is noticeable â€“ enough to miss a target in a dogfight." >}}


## Fixed-point & Integer math

Floatingâ€‘point numbers give you huge ranges, but they are nonâ€‘deterministic across CPUs and they waste memory/bandwidth (8â€¯bytes for a double).
In a massive multiplayer game like Starâ€¯Citizen you often need exactly the same result on every client and you want to keep packets as small as possible.
Fixedâ€‘point solves both problems by storing a fractional value as an ordinary integer multiplied by a constant scale factor.

**How it works**

Pick a powerâ€‘ofâ€‘two scale (e.g., `SCALE = 256`, i.e. 8 fractional bits).

```
stored = round(real_value * SCALE)   # integer stored in memory
real   = stored / SCALE              # convert back when you need to display
```

All arithmetic is then plain integer mathâ€”no hidden rounding, no platformâ€‘dependent quirks.

**Tiny Python demo (16-bit, 8-bit fraction)**

```python
SCALE = 256                # 2â¸ â†’ each step â‰ˆ 0.0039
def to_fixed(v): return int(round(v * SCALE))
def to_float(i): return i / SCALE

shield = to_fixed(0.45)    # 45â€¯% shield
for _ in range(5):
    shield = min(shield + to_fixed(0.0075), 2**15-1)   # regen @0.75â€¯%/s
    print(to_float(shield))
```

Output (rounded): 0.4578, 0.4657, 0.4736, 0.4815, 0.4894 â€“ a deterministic regeneration curve that would be identical on every machine.

Benefits for game engines:

- Determinism: All clients compute the same value, eliminating desync bugs.
- Smaller payloads: A 16â€‘bit fixedâ€‘point field (2â€¯bytes) replaces a 32â€‘bit float (4â€¯bytes), halving the bandwidth needed for things like ship velocity or fuel level.
- Cacheâ€‘friendliness: Smaller structs fit better in CPU caches, helping maintain high frame rates during crowded dogfights.

Tradeâ€‘offs:

- Limited range: With a 16â€‘bit signed value and SCALE = 256 you can only represent numbers up to â‰ˆâ€¯128. Larger ranges require a wider integer or a smaller scale.
- Quantisation: The smallest step you can represent is 1/SCALE (â‰ˆâ€¯0.004 in the example). Very fineâ€‘grained physics may need a finer scale, which reduces the maximum representable value.

In short, fixedâ€‘point is the pragmatic middle ground: you keep the memory and network efficiency of integers while still being able to work with
fractionsâ€”exactly what a sprawling, networked space sim needs.

{{< figure src="float_vs_fixed.png" alt="Floating point vs Fixed point" caption="Floating point vs Fixed point memory footprint." >}}


### Is this really the only alternative real number representation?

Fixedâ€‘point isnâ€™t the only way to store real numbersâ€”different useâ€‘cases call for different tricks. Take inâ€‘game currency, for instance.
When you need exact currency values, a common shortcut is to store the wholeâ€‘unit part and the fractionalâ€‘cent part as two separate integers (e.g., credits = 1234, cents = 56).
This avoids any floatingâ€‘point rounding quirks while keeping the representation simple and networkâ€‘friendly.

## Double precision coordinates in Star Citizen
Cloudâ€¯Imperiumâ€¯Games has confirmed that the engine now uses 64â€‘bit (doubleâ€‘precision) floatingâ€‘point values for worldâ€‘space coordinates[^2].
Seanâ€¯Tracy (Technical Director) explained that moving from 32â€‘bit to 64â€‘bit â€œallows greater precision and size for positional spaceâ€[^3]
because the gameâ€™s star systems span millions of kilometres, far beyond what a 32â€‘bit float can accurately represent.

**Why it matters**

- **Precision at astronomical scales** â€“ With double precision the smallest distinguishable distance (the ulp) stays subâ€‘metre even when coordinates reach billions of metres, preventing the jitter you sometimes see when entering a ship.
- **Deterministic physics** â€“ Higher precision reduces cumulative rounding errors in trajectory calculations, essential for accurate combat, docking, and navigation across vast distances.
- **Performace trade-offs** â€“ Doubleâ€‘precision values occupy 8â€¯bytes each, twice the size of a 32â€‘bit float. That extra space means:
  - Every read or write moves double the amount of data across the memory bus, which can become a bottleneck when the engine has to update millions of positions each frame.
  - Because the CPUâ€™s L1/L2 caches are limited (typically a few megabytes), larger structures mean fewer position entries fit in cache, leading to more cacheâ€‘misses and a measurable dip in frameâ€‘rate.
  - (My guess) The engine therefore reserves doubleâ€‘precision only for the global worldâ€‘space coordinates that need astronomical range and subâ€‘metre precision. For anything that the player actually sees on screenâ€”nearby ships, terrain patches, UI elementsâ€”it falls back to 32â€‘bit floats, which are natively supported by GPUs and consume half the bandwidth. As Seanâ€¯Tracy explains:
  > GPUs themselves generally donâ€™t work in 64â€‘bit â€“ they work in 32â€‘bit â€“ but the visible range stays inside 32â€‘bits while the overall system space is much bigger.

  In short, the engine balances precision against performance by using doubles where the sheer size of the universe demands it, and floats where speed and bandwidth matter most.

## So what have learned this week?

- **Integers are finite.** Donâ€™t assume you can count arbitrarily many trolleys; youâ€™ll hit the 2Â³Â¹â€‘1 ceiling sooner or later.
- **Floats give range, not exactness.** Expect gaps that widen as numbers grow â€“ the â€œrealâ€‘worldâ€ you simulate is always a bit fuzzy.
- **Overflow and rounding arenâ€™t bugs; theyâ€™re physics.** They explain why a perfectly timed jump sometimes lands you inside an asteroid.
- **Fixedâ€‘point can be your friend when you need deterministic**, lowâ€‘memory math (think serverâ€‘side state sync).
- **Doubleâ€‘precision** coordinates let the universe be big but come with memory and bandwidth costs that the engine must manage.

So next time you stare at a UI element that says â€œFuel: 99.999â€¯%â€, remember: the computer is guessing that last fraction, and the guess might be off by a hair.
In the unforgiving vacuum of space, that hair can be the difference between a clean landing and a spectacular crashâ€‘landing into a rockâ€‘filled nebula.

Happy coding, and may your bits never overflowâ€”unless you enjoy watching your cargo counter roll over into negative infinity. ğŸš€

[^0]: https://en.wikipedia.org/wiki/Integer_overflow
[^1]: https://en.wikipedia.org/wiki/IEEE_754
[^2]: https://gamersnexus.net/gg/2622-star-citizen-sean-tracy-64bit-engine-tech-edge-blending
[^3]: https://www.reddit.com/r/starcitizen/comments/7nj5yl/star_citizen_64bit_and_double_precision_floating/
[^log]: https://en.wikipedia.org/wiki/Logarithmic_scale
